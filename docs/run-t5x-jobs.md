# Run and monitor T5X jobs with Vertex AI

T5X exposes three top-level run functions: `train`, `evaluate`, and `infer`. You
use the `train` function for both pretraining and fine-tuning. You use the
`evaluate` function for evaluating and the `infer` function inferring. You can
parametrize these functions and any object that is passed to them using the [Gin
configuration framework](https://github.com/google/gin-config). Using Gin gives
you flexibility in configuring your training and inference tasks without the
need to modify any code within the core T5X library. For more information about
how T5X uses Gin, see the
[T5X Gin Primer](https://github.com/google-research/t5x/blob/main/docs/usage/gin.md).

T5X uses the [SeqIO
library](https://github.com/google/seqio/blob/main/README.md) for processing
data that will be fed into models for training, inference, and evaluation. SeqIO
encapsulates data access, preprocessing, postprocessing, and evaluation metrics
in reusable components called Tasks. (For clarity, in this document we always
use the term SeqIO Task when referring to a task in the SeqIO context.)

Although T5X supports GPU acceleration, it's optimized for [Cloud
TPUs](https://cloud.google.com/tpu). The guidance in this repo focuses on
showing how to use T5X with Cloud TPUs in Vertex AI.

Configuring and running a T5X job in Vertex AI consists of the following
high-level steps:

1. You choose a model to pretrain, fine-tune, evaluate, or run inference
    on. The model is defined by a model Gin file and a checkpoint. In most
    cases, you use one of the
    [predefined Gin files](https://github.com/google-research/t5x/tree/main/t5x/examples)
    and
    [pretrained checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md)
    in the T5X repo.
1. You choose a base run Gin file for your job. T5X provides a
    [set of predefined run Gin files](https://github.com/google-research/t5x/tree/main/t5x/configs/runs)
    that encapsulate the default settings for fine-tuning, pretraining,
    evaluating, or inferring runs.
1. You choose the SeqIO Task for your scenario. You can either choose one of
    the
    [predefined SeqIO Tasks](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/data/tasks.py)
    or you can define your own Task.
1. You create a Gin file that defines your task. Your Gin file refers to the
    model Gin file and to the base run Gin file. In your Gin file for the task,
    you provide the required parameters or you overwrite the default parameters
    from the model Gin file and from the run Gin file.
1. You start the job by running the T5X launch script (`main.py`). You
    specify the function to run (`train`, `eval`, `infer`, or `compile`) and
    the Gin file to use as command-line parameters. Your Gin file (and any
    command line overrides) parametrize the run function.

The following sections provide background information for how this process can
be implemented using Vertex AI.

## Custom training container for T5X

The approach that's used in this repo uses
[Vertex AI custom training jobs](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)
with a
[custom training container](https://cloud.google.com/vertex-ai/docs/training/containers-overview)
to run the T5X training tasks, the evaluation tasks, and the inference tasks.
The
[base custom training container image](https://github.com/GoogleCloudPlatform/t5x-on-vertex-ai/blob/master/Dockerfile)
packages the T5X framework and its dependencies and sets the default entry point
to the T5X launch script. Most samples in the repo use the base custom training
container image directly. Some samples use an example container image that's
derived from the base image.

## Set custom job specification for T5X runs

When you create a custom job, you specify
[settings](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec)
that Vertex AI needs in order to run your code, including one or more worker
pool specifications. All T5X job types—training, evaluating, and inferring—use
the same base configuration of a worker pool specification. The following
listing shows an example of a specification; the specification is generated by a
function, as discussed later in this document.

```
worker_pool_specs: [{
  machine_spec: {
    machine_type: "cloud-tpu",
    accelerator_type: TPU_V2,
    accelerator_count: 128,
  },
  replica_count: 1,
  container_spec: {
    image_uri: "gcr.io/<GOOGLE_PROJECT_ID>/t5x-base",
    args: [
        "--run_mode=train",
        "--gin_file=/gcs/<PATH_TO_GIN_FILE>",
        "--gin.MODEL_DIR=\"gs://<BUCKET>/<MODEL_DIR>\"",
        "--gin.EVALUATOR_NUM_EXAMPLES=500,
    ]
  }
}]
```

Because T5X training, evaluating, and inferring tasks run on Cloud TPUs, the
`machine_spec` specification describes a Cloud TPU configuration. Therefore, the
`machine_type` field is always set to `cloud-tpu`. The `accelerator_type` field
is set to the Cloud TPU version. Vertex AI supports TPU v2 (`TPU_V2`) and TPU v3
(`TPU_V3`). The `accelerator_count` field specifies the requested number of TPU
cores. TPUs are available on a
[region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)
basis, and you must verify that the configuration that you require is available
in your region. The `replica_count` field is always set to 1.

The `container_spec` specification declares the
[command-line parameters](https://github.com/google-research/t5x/blob/main/t5x/main.py)
that will be passed to the T5X launch script (`main.py`). You always need to set
the `--run_mode` and `--gin_file` parameters. You can set the `--run_mode
`parameter to `train`, `eval`, `infer`, or `precompile`. The `--gin_file`
parameter must be set to the location of your Gin file on Google Cloud Storage,
specified using the
[Cloud Storage FUSE](https://cloud.google.com/vertex-ai/docs/training/code-requirements#fuse)
path format. For example, if your Gin file is in `gs://BUCKET/t5_wmt.gin`, you
set `--gin_file` to `/gcs/BUCKET/t5_wmt.gin`.  

If your Gin file or any Gin files are referenced by your Gin define
`gin.REQUIRED` macros, you must provide the missing values using Gin command
overrides. You can also change the default value of any setting in the Gin file
using a Gin command override. A Gin command override has the following format:  

`--gin.<PARAMETER_NAME>=<PARAMETER_VALUE>`   

In the earlier example, `--gin.MODEL_DIR` and `--gin.EVALUATOR` are Gin command
overrides.  

The repo includes the
[`create_t5x_custom_job` utility function](https://github.com/GoogleCloudPlatform/t5x-on-vertex-ai/blob/38e288538c7180eff863ca4232756c0bef317ca9/notebooks/utils.py#L36)
that streamlines the configuration of a Vertex AI custom job. The function
copies your Gin files into a staging area in Cloud Storage and returns a
properly configured custom job specification. directly. All example notebooks in
this repo use the `create_t5x_custom_job` function.

## Use Vertex AI Experiments to track T5X jobs

T5X jobs require complex configuration. They often run on large TPU slices, and
they generate ample metadata and artifacts. To support reproducibility and
effective run monitoring and analysis, you need to track model configurations,
training parameters, datasets, checkpoints, metrics, and more. You can track
these items using
[Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments).  

In the code samples in the repo, Vertex AI Experiments is used to track the
following:

-  The run configuration. A run config Gin file that aggregates all run
    settings, including command overrides is tracked as a 
    [Vertex AI Experiments artifact](https://cloud.google.com/vertex-ai/docs/experiments/track-executions-artifacts).

-  The SeqIO Task and dataset that are used by your job. These items are
    tracked as an Vertex AI Experiments artifact of the `Dataset` type.
-  Metrics that are generated by the run. All metrics that are generated by
    T5X are tracked using
    [Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview).
    In addition, a subset of metrics that are defined in the SeqIO Task
    evaluate the effectiveness of a model on a given NLP task. This subset of
    metrics is extracted from TensorBoard logs and promoted as Vertex AI
    Experiments summary metrics.
-  The Vertex AI Training custom job that runs your T5X track. The custom
    job is tracked as a
    [Vertex AI Experiments execution](https://cloud.google.com/vertex-ai/docs/experiments/track-executions-artifacts)
    and links all metadata and artifacts that are created by the run into a
    lineage graph.

The repo includes the
[`submit_and_track_t5x_vertex_job` utility function](https://github.com/GoogleCloudPlatform/t5x-on-vertex-ai/blob/370c2aa8315fb9e46af47b095e8f58a97a021896/notebooks/utils.py#L191)
that unifies tracking and job submission into a single function call. All
example notebooks use this function to start T5X Vertex AI Training jobs.

## Use Vertex AI TensorBoard

Vertex AI custom training features
[integration with Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-training).
However, Vertex AI TensorBoard is not fully supported when you use custom
training with Cloud TPUs. You can mitigate this constraint by using
[Vertex AI TensorBoard Uploader](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview#upload-tb-logs).  

The example notebooks show you how to set up continuous uploading of TensorBoard
logs that are generated by T5X to Vertex AI TensorBoard.
